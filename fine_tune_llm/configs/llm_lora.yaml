# LoRA SFT configuration for LLM fine-tuning
# Multi-model support
model_options:
  glm-4.5-air:
    model_id: ZHIPU-AI/glm-4-9b-chat
    tokenizer_id: ZHIPU-AI/glm-4-9b-chat
    target_modules: ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
    chat_template: "glm"
  qwen2.5-7b:
    model_id: Qwen/Qwen2.5-7B
    tokenizer_id: Qwen/Qwen2.5-7B
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    chat_template: "qwen"
  mistral-7b:
    model_id: mistralai/Mistral-7B-v0.1
    tokenizer_id: mistralai/Mistral-7B-v0.1
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
    chat_template: "mistral"
  llama-3-8b:
    model_id: meta-llama/Meta-Llama-3-8B
    tokenizer_id: meta-llama/Meta-Llama-3-8B
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    chat_template: "llama"

selected_model: glm-4.5-air  # Default

lora:
  # LoRA/DoRA/AdaLoRA parameters
  method: lora  # Options: lora, dora, adalora
  r: 16  # Rank (initial rank for AdaLoRA)
  lora_alpha: 32
  lora_dropout: 0.1
  
  # AdaLoRA specific parameters
  adalora:
    target_r: 8  # Target rank after pruning
    init_r: 12   # Initial rank (if different from r)
    tinit: 0     # Warmup steps before pruning
    tfinal: 0    # Steps after which stop pruning (0 = auto)
    deltaT: 1    # Update frequency for importance score
  
  # QLoRA quantization settings
  quantization:
    enabled: false  # Default off for backward compatibility
    bits: 4  # Options: 4 or 8
    compute_dtype: bfloat16  # For mixed precision
    double_quant: true  # Extra memory savings
    quant_type: nf4  # Normalized float4
    
# Training configuration
training:
  # Basic parameters
  learning_rate: 2e-4
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4
  warmup_ratio: 0.03
  weight_decay: 0.001
  max_grad_norm: 0.3
  
  # Learning rate scheduler
  scheduler:
    type: cosine  # Options: cosine, linear, none
    warmup_steps: 100  # 10% of total steps recommended
  
  # Sequence parameters
  max_length: 2048
  padding: true
  truncation: true
  
  # Training optimizations
  fp16: false
  bf16: true  # Use bfloat16 for better stability
  gradient_checkpointing: true
  optim: paged_adamw_32bit
  
  # Data configuration
  packing: true  # Pack multiple examples per sequence
  num_proc: 4  # Parallel data processing
  
  # Evaluation
  eval_steps: 100
  save_steps: 200
  logging_steps: 10
  eval_strategy: steps
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: eval_f1
  greater_is_better: true
  
  # Early stopping
  early_stopping: true
  early_stopping_patience: 3
  
  # Logging and monitoring
  logging:
    wandb: false  # Enable Weights & Biases logging
    project_name: "llm-finetuning"
    entity: null  # W&B team/entity name (optional)
    run_name: null  # Auto-generated if null
    tags: ["lora", "finetuning"]
    notes: "LLM fine-tuning experiment"

# Evaluation configuration
evaluation:
  enabled: true
  metrics: ["accuracy", "f1"]
  val_split: 0.1  # Fraction of data for validation

# Data augmentation
data:
  augmentation:
    enabled: false
    methods: ["synonym_replacement", "random_insertion"]
    aug_p: 0.1  # Probability of augmentation

# Output configuration
output:
  structured: false  # Enable for enforced JSON
  schema: '{"decision": "str", "rationale": "str", "abstain": "bool"}'  # JSON schema
  
# Instruction format
instruction_format:
  system_prompt: |
    You are a specialized classifier for bird flu content. 
    Analyze the text and return a JSON response with your classification.
    You MUST return valid JSON with "decision", "rationale", and "abstain" fields.
    If you are uncertain or the text is ambiguous, set "abstain": true.
    
  input_template: |
    Text to classify:
    {text}
    
    Metadata:
    {metadata}
    
  output_template: |
    {
      "decision": "{label}",
      "rationale": "{rationale}",
      "abstain": {abstain}
    }
    
  abstain_examples:
    - text: "The document appears corrupted with mixed languages and symbols..."
      output: |
        {
          "decision": null,
          "rationale": "Text appears corrupted or unintelligible",
          "abstain": true
        }
    - text: "This might be about birds or it might be about flu but I'm not sure..."
      output: |
        {
          "decision": null,
          "rationale": "Ambiguous content without clear indicators",
          "abstain": true
        }