# LoRA SFT configuration for LLM fine-tuning
# Multi-model support
model_options:
  glm-4.5-air:
    model_id: ZHIPU-AI/glm-4-9b-chat
    tokenizer_id: ZHIPU-AI/glm-4-9b-chat
    target_modules: ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
    chat_template: "glm"
  qwen2.5-7b:
    model_id: Qwen/Qwen2.5-7B
    tokenizer_id: Qwen/Qwen2.5-7B
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    chat_template: "qwen"
  mistral-7b:
    model_id: mistralai/Mistral-7B-v0.1
    tokenizer_id: mistralai/Mistral-7B-v0.1
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
    chat_template: "mistral"
  llama-3-8b:
    model_id: meta-llama/Meta-Llama-3-8B
    tokenizer_id: meta-llama/Meta-Llama-3-8B
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    chat_template: "llama"

selected_model: glm-4.5-air  # Default

lora:
  # LoRA/DoRA/AdaLoRA parameters
  method: lora  # Options: lora, dora, adalora
  r: 16  # Rank (initial rank for AdaLoRA)
  lora_alpha: 32
  lora_dropout: 0.1
  
  # AdaLoRA specific parameters
  adalora:
    target_r: 8  # Target rank after pruning
    init_r: 12   # Initial rank (if different from r)
    tinit: 0     # Warmup steps before pruning
    tfinal: 0    # Steps after which stop pruning (0 = auto)
    deltaT: 1    # Update frequency for importance score
  
  # QLoRA quantization settings
  quantization:
    enabled: false  # Default off for backward compatibility
    bits: 4  # Options: 4 or 8
    compute_dtype: bfloat16  # For mixed precision
    double_quant: true  # Extra memory savings
    quant_type: nf4  # Normalized float4
    
# Training configuration
training:
  # Basic parameters
  learning_rate: 2e-4
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4
  warmup_ratio: 0.03
  weight_decay: 0.001
  max_grad_norm: 0.3
  
  # Learning rate scheduler
  scheduler:
    type: cosine  # Options: cosine, linear, none
    warmup_steps: 100  # 10% of total steps recommended
  
  # Sequence parameters
  max_length: 2048
  padding: true
  truncation: true
  
  # Training optimizations
  fp16: false
  bf16: true  # Use bfloat16 for better stability
  gradient_checkpointing: true
  optim: paged_adamw_32bit
  
  # Data configuration
  packing: true  # Pack multiple examples per sequence
  num_proc: 4  # Parallel data processing
  
  # Evaluation
  eval_steps: 100
  save_steps: 200
  logging_steps: 10
  eval_strategy: steps
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: eval_f1
  greater_is_better: true
  
  # Early stopping
  early_stopping: true
  early_stopping_patience: 3
  
  # Logging and monitoring
  logging:
    wandb: false  # Enable Weights & Biases logging
    project_name: "llm-finetuning"
    entity: null  # W&B team/entity name (optional)
    run_name: null  # Auto-generated if null
    tags: ["lora", "finetuning"]
    notes: "LLM fine-tuning experiment"

# Evaluation configuration
evaluation:
  enabled: true
  metrics: ["accuracy", "f1"]
  val_split: 0.1  # Fraction of data for validation

# Data augmentation
data:
  augmentation:
    enabled: false
    methods: ["synonym_replacement", "random_insertion"]
    aug_p: 0.1  # Probability of augmentation

# Output configuration
output:
  structured: false  # Enable for enforced JSON
  schema: '{"decision": "str", "rationale": "str", "abstain": "bool"}'  # JSON schema
  
# Instruction format
instruction_format:
  system_prompt: |
    You are a specialized classifier for bird flu content. 
    Analyze the text and return a JSON response with your classification.
    You MUST return valid JSON with "decision", "rationale", and "abstain" fields.
    If you are uncertain or the text is ambiguous, set "abstain": true.
    
  input_template: |
    Text to classify:
    {text}
    
    Metadata:
    {metadata}
    
  output_template: |
    {
      "decision": "{label}",
      "rationale": "{rationale}",
      "abstain": {abstain}
    }
    
  abstain_examples:
    - text: "The document appears corrupted with mixed languages and symbols..."
      output: |
        {
          "decision": null,
          "rationale": "Text appears corrupted or unintelligible",
          "abstain": true
        }
    - text: "This might be about birds or it might be about flu but I'm not sure..."
      output: |
        {
          "decision": null,
          "rationale": "Ambiguous content without clear indicators",
          "abstain": true
        }

# High-Stakes Precision and Auditability Features
high_stakes:
  # Uncertainty-aware fine-tuning for precision
  uncertainty:
    enabled: false  # Enable for high-stakes mode
    method: "mc_dropout"  # Options: mc_dropout, deep_ensemble
    num_samples: 5  # Forward passes for estimation
    abstention_threshold: 0.7  # Abstain if uncertainty > this
    fp_penalty_weight: 2.0  # Heavy penalty for overconfident false positives
    
  # Factual accuracy enhancement with RELIANCE
  factual:
    enabled: false
    reliance_steps: 3  # Number of intermediate reasoning steps to check
    fact_penalty_weight: 2.0  # Heavy for high-stakes precision
    self_consistency_threshold: 0.8  # Agreement threshold for factual claims
    
  # Bias auditing for fairness
  bias_audit:
    enabled: false
    audit_categories: ["gender", "race", "age", "nationality"]
    bias_threshold: 0.1  # Maximum allowed bias score
    mitigation_weight: 1.5  # Weight for bias mitigation in loss
    
  # Explainable reasoning
  explainable:
    enabled: false
    chain_of_thought: true  # Require step-by-step reasoning
    reasoning_steps: 3  # Minimum reasoning steps
    faithfulness_check: true  # Verify reasoning leads to conclusion
    
  # Procedural alignment for domains
  procedural:
    enabled: false
    domain: "medical"  # Options: medical, legal, financial
    procedure_file: "configs/procedures.yaml"  # Domain-specific procedures
    compliance_weight: 2.0  # Weight for procedure compliance
    
  # Verifiable training provenance
  verifiable:
    enabled: false
    hash_artifacts: true  # Hash all training artifacts
    cryptographic_proof: true  # Generate cryptographic proofs
    audit_log: "artifacts/audit_trail.jsonl"  # Audit trail file
    checkpoint_hashing: true  # Hash each checkpoint

# Advanced metrics and conformal prediction
advanced_metrics:
  # Conformal prediction for uncertainty quantification
  conformal_prediction:
    enabled: false  # Enable for advanced uncertainty handling
    alpha: 0.1  # Miscoverage level (90% coverage)
    method: "lac"  # Options: lac (Least Ambiguous Set), aps (Adaptive Prediction Sets)
    calibration_ratio: 0.2  # Fraction of data for calibration
    
  # Risk-controlled prediction
  risk_control:
    enabled: false  # Enable for high-stakes applications
    alpha: 0.1  # Risk level
    cost_matrix: "default"  # Options: default, custom
    class_names: ["HIGH_RISK", "MEDIUM_RISK", "LOW_RISK", "NO_RISK"]
    
  # Abstention-aware loss function
  abstention_loss:
    enabled: false  # Enable abstention-aware training
    confidence_threshold: 0.7  # Threshold for high confidence
    abstention_penalty: 0.3  # Penalty for low-confidence predictions
    uncertainty_weight: 0.1  # Weight for entropy regularization
    
  # Calibration metrics tracking
  calibration_tracking:
    enabled: false  # Enable ECE/MCE tracking during training
    learning_rate_adjustment: false  # Adjust LR based on calibration drift
    adjustment_factor: 0.9  # LR reduction factor
    adjustment_patience: 3  # Epochs to wait before adjustment